\section{Mesures d'adéquation et sélection de modèle }\label{Sect_Adequation}	
Rappelons que l'objectif de ce travail est de déterminer quel est le modèle le plus adéquat pour chacun des jeux de données. Ainsi, afin de l'identifier dans chaque contexte, il faut déterminer des critères de comparaison. Pour y arriver, \cite{LossModels_Klugman2012}(p.323-350) décrit plusieurs critères et propose une méthodologie de sélection de modèle.\\

La comparaison des modèles débute par une analyse graphique puisqu'il s'agit de la façon la plus directe de voir si le modèle décrit adéquatement les données. À cette effet, à l'aide des graphiques de densité de probabilité, des fonctions de répartition et des graphiques de quantiles à quantiles (\textit{QQplots}), il est possible de vérifier cette adéquation. Cependant il n'est pas nécessaire de tous les faire. De cette façon, comme le graphique le plus facile à comparer est le \textit{QQplot}, c'est lui qui est utiliser pour le présent travail.\\

Par la suite, afin d'avoir des mesures tangibles pour comparer les modèles, différents tests et critères de comparaison sont proposés par \cite{LossModels_Klugman2012}. Mentionnons le test de Kolmogorov-Smirnov, celui d'Anderson-Darling et les critères d'Akaike et de Bayes. La présente section décrit donc ces tests en vue de faire une analyse adéquate.

En ce qui attrait aux deux premiers tests, il s'agit de calculer l'adéquation du modèle en testant les hypothèses suivantes:
\begin{description}
	\item[$H_0$:] Le modèle représente bien les données de l'échantillon.
	\item[$H_1$:] Le modèle ne représente pas bien les données de l'échantillon.\\
\end{description}

\subsection{Test de Kolmogorov-Smirnov}
Le test de Kolmogorov-Smirnov vient compléter l'analyse graphique du fait qu'il mesure la distance entre chaque points de la fonction de répartition empirique avec la courbe de la fonction de répartition théorique. De cette façon, lorsque l'on hésite entre deux graphiques qui semblent similaires, cette mesure vient départager celui qui reproduit le mieux la fonction de répartition empirique.

	\begin{Definition}
	Soient $F_X(x)$, la fonction de répartition théorique de $X$ et $\widehat{F_N}(x)$, la fonction empirique dont on essaie de reproduire le comportement. Il faut que $F_X$ soit continue et que les données ne soient pas groupées.	Alors, la statistique de Kolmogorov-Smirnov est définie par 
	\begin{align}
	D = max \left(\sup_{d\leq x_i}\limits \left\lbrace \left|  \widehat{F_N}(x^-)-F_X(x) \right|\right\rbrace , \sup_{d\leq x_i}\limits\left\lbrace\left| \widehat{F_N}(x)-F_X(x) \right|   \right\rbrace  \right),
	\end{align}
	
	où d correspond au montant tronqué en début de distribution et $\widehat{F_N}(x^-)$ correspond est la valeur de la fonction de répartition empirique qui se trouve juste avant le saut. 
	\end{Definition}

	On calcule la distance maximale ($D$) qui existe entre la fonction de répartition théorique et la fonction de répartition théorique.\\
		
	Cependant, on ne connaît pas la distribution de la statistique $D$. Malgré tout, dans \cite{LossModels_Klugman2012}(p.330-332), les auteurs définissent une méthode permettant de calculer l'adéquation du modèle. Cette méthode est illustrée dans la proposition \ref{Propo_ValeursCritiques_Kolmogorov}.
	
	\begin{Proposition}\label{Propo_ValeursCritiques_Kolmogorov}
	Soit $c_\alpha$, tel que $P[D>c_\alpha]=\alpha$ et $n$, la taille de l'échantillon. Alors les valeurs critiques sont définies dans le tableau \ref{TBL_ValeursCritiques_Kolmogorov}.
		
		\begin{table}[H]
			\begin{center}
			\begin{TAB}(r,2cm){|c|cccc|}{|c|c|}
				$\alpha$ & 20\% & 10\% & 5\% & 1\%\\
				$c_\alpha$ & $\frac{1.07}{\sqrt n}$ & $\frac{1.22}{\sqrt n}$ & $\frac{1.36}{\sqrt(n)}$ & $\frac{1.63}{\sqrt n}$\\
			\end{TAB}
			\renewcommand{\tablename}{Tableau}
			\caption{Test de Kolmogorov-Smirnov: Valeurs critiques.} \label{TBL_ValeursCritiques_Kolmogorov}
			\end{center}
		\textbf{Remarque:}\textit{Ces approximations des valeurs critiques sont bonnes si n $>$ 15.}
		\end{table}	
	\end{Proposition}
	
	Dans \texttt{R}, la fonction \texttt{ks.test} effectue directement ce test. Cette dernière renvoi la distance maximale ($D$) ainsi qu'une \textit{valeur $P$}. Pour en apprendre davantage, on peut consulter \url{https://www.rdocumentation.org/packages/dgof/versions/1.2/topics/ks.test}. On y trouve toutes les références ayant servi à construire la formule et à déterminer la \textit{valeur $P$}.
	
\subsection{Test d'Anderson-Darling}
	Afin de mesurer l'efficacité d'un modèle, le test de Kolmogorov-Smirnov est peu performant pour repérer les problèmes dans les queues de distribution. Pour cette raison, on utilise le test d'Anderson-Darling défini dans \cite{LossModels_Klugman2012}(p.332-333) pour remédier à ce problème.
	\begin{Definition}
		Soient $F_X(x)$, la fonction de répartition théorique de $X$ et $\widehat{F_N}(x)$, la fonction empirique dont on essaie de reproduire le comportement. La statistique d'Anderson-Darling se définit comme 
		\begin{align}\label{Stat_AnedersonDarling}
		A^2 = n \int_{d}^{\infty}\frac{\left( \widehat{F_N}(x)-F_X(x)\right)^2}{F_X(x) \left(1-F_X(x)\right)} \textrm{d}F_X(x),
		\end{align}

	 où d correspond au montant tronqué en début de distribution et $\widehat{F_N}(x^-)$ correspond est la valeur de la fonction de répartition empirique qui se trouve juste avant le saut.
	\end{Definition}

	L'idée derrière la statistique en \ref{Stat_AnedersonDarling} est de calculer une moyenne pondérée du carré des écarts entre la fonction de répartition empirique et son homologue théorique.
	Comme des valeurs de $x$ qui sont petites ou très grandes renvoient des valeurs plus grandes dans le dénominateur, davantage de poids est accordé à ces valeurs. C'est pour cette raison que le test d'Anderson-Darling permet de bien évaluer les queues de distribution.\\
	
	\textbf{Remarque:} \textit{Encore ici, ce test ne fonctionne qu'avec des données non-groupées.}\\
	
	De la même façon que pour le test de Kolmogorov-Smirnov, \cite{LossModels_Klugman2012} définit les valeurs critiques de référence qui sont présentés dans la proposition \ref{Propo_ValeursCritiques_Anderson}.
	
	\begin{Proposition}\label{Propo_ValeursCritiques_Anderson}
	Soit $c_\alpha$, tel que $P[A^2>c_\alpha]=\alpha$. Alors les valeurs critiques sont fournies dans le tableau \ref{TBL_ValeursCritiques_Anderson}.
	
	\begin{table}[H]
		\begin{center}
			\begin{TAB}(r,2cm){|c|ccc|}{|c|c|}
				$\alpha$ 	& 10\% 	& 5\% 	& 1\%\\
				$c_\alpha$ 	& 1.933	& 2.492 &  3.857\\
			\end{TAB}
			\renewcommand{\tablename}{Tableau}
			\caption{Test d'Anderson-Darling: Valeurs critiques.} \label{TBL_ValeursCritiques_Anderson}
		\end{center}
	\end{table}	
	\end{Proposition}
	
	Du point de vue informatique, afin de pouvoir calculer la statistique, il faut modifier \ref{Stat_AnedersonDarling} afin de ne pas avoir à évaluer d'intégrale. On obtient donc 
	\begin{align}\label{Stat_AnedersonDarling2}
	A^2 &= -n \left[ 1-\sum_{j=1}^{k} \widehat{F_N^2}(x_{[j]}) \ln\left( \frac{F_X(x_{[j+1]})}{F_X(x_{[j]})}  \right) + \sum_{j=0}^{k-1} (1-\widehat{F_N}(x_{[j]}))^2 \ln\left( \frac{1-F_X(x_{[j+1]})}{1-F_X(x_{[j]})} \right) \right] \nonumber \\
	&= -n \left[ 1-\sum_{j=1}^{k} (j/n)^2 \ln\left( \frac{F_X(x_{[j+1]})}{F_X(x_{[j]})}  \right) + \sum_{j=0}^{k-1} (1-j/n)^2 \ln\left( \frac{1-F_X(x_{[j+1]})}{1-F_X(x_{[j]})} \right) \right]
	\end{align}
	où $x_{[j]}$ est la $j^e$ statistique d'ordre de $X$, $n$ est le nombre total d'observations et $k$ est le nombre d'observations uniques.

\subsection{Critère d'information d'Akaike}
	Une fois que les tests d'adéquation ont été effectués, il peut arriver que des valeurs soient comparables ou peu concluantes. À ce moment, dans la littérature, plusieurs auteurs mentionnent les critères de sélection AIC et BIC pour départager les modèles. Parmi les ouvrages y faisant référence, mentionnons \cite{Parodi2015_Pricing_in_GenIns}(p.466-467) et \cite{albrecher2017reinsurance}(p.98-99).\\
	
	Ainsi, le critère d'information d'Akaike est définit dans \cite{Akaike_Criteria1974} comme étant
	\begin{align}
	AIC =  2k-2 \ln \mathcal{L}(\theta);
	\end{align}
	où $\mathcal{L}(\theta)$ est la fonction de vraisemblance du modèle et $k$ est le nombre de paramètres.\\
	
	L'objectif étant de minimiser la log-vraisemblance négative en pénalisant pour le nombre de paramètres. De cette façon, l'AIC considère à la fois la qualité de prédiction du modèle et sa complexité. On sélectionnera donc un modèle qui minimisera l'AIC.

\subsection{Critère bayésien de Schwartz}
	Un critère alternatif au critère d'information d'Akaike est le critère bayésien de Schwartz (BIC). En effet, Gideon E. Shwartz a définit un critère d'information alternatif à Akaike qui pénalise davantage un modèle en fonction du nombre de paramètres. Ainsi, il définit dans \cite{schwarz1978} le BIC comme étant 	
	\begin{align}
	BIC =  k\ln n - 2\ln \mathcal{L}(\theta);
	\end{align}
	où $n$ correspond au nombre d'observations dans l'échantillon.\\
	
	De cette façon, en comparant le BIC à l'AIC, il est possible d'obtenir des résultats différents selon que l'on veuille un modèle plus simple ou un autre qui maximise davantage la vraisemblance. Cependant, cette différence sera minime si le nombre d'observations dans l'échantillon de données est faible.
	
\subsection{Test du ratio de vraissemblance}
		Une fois que des modèles ont été sélectionnés à la suite de l'analyse globale des tests et critères précédents, il reste à déterminer si la simplification d'un modèle proposé serait une alternative tout aussi valable que le modèle proposé. Pour y parvenir, il suffit de calculer le ratio de vraisemblance tel qu'expliqué dans \cite{LossModels_Klugman2012}(p.331-339).
		
		\begin{Definition}
			Considérons deux modèles dont l'un est une simplification de l'autre. Soient les hypothèses
			\begin{description}
				\item $H_0\ :$ \itshape {Le modèle le plus complexe n'ajoute rien au modèle le plus simple;}
				\item $H_1\ :$ \itshape {Le modèle le plus complexe permet de mieux maximiser la vraisemblance du modèle;}\\
			\end{description}
			et soient $\mathcal{L}(\theta_0)$, la fonction de vraisemblance du modèle simple et $\mathcal{L}(\theta_1)$, celle du modèle plus complexe.\\
			
			On définit la statistique $R = 2 \left\lbrace \ln \mathcal{L}(\theta_0 ) - \ln( \mathcal{L}( \theta_1))\right\rbrace $, suivant une loi Khi-carrée, avec un nombre de degrés de liberté correspondant à la différence de nombres de paramètres entre les deux modèles. \\
			
			On rejette $H_0$ si $R>X^{2}_{\kappa}(dl)$, où $X^{2}_{\kappa}(dl)$ est le $\kappa^e$ quantile d'une loi khi-carré avec $dl$ degrés de libertés.
		\end{Definition}
	
		Par cette méthode, il est possible de vérifier si un processus de Poisson non-homogène est significativement plus approprié d'un processus de Poisson homogène. Dans le même état d'esprit, on peut également utiliser ce test pour valider s'il vaut la peine d'ajouter des raccordements à une loi de sévérité.
	
\subsection{Sélection de modèle}
	Une fois l'analyse graphique effectuée et les critères de comparaison calculés, il reste à choisir le modèle le plus adéquat. Cependant, comme l'explique \cite{LossModels_Klugman2012}, il n'y a pas de science infuse pour sélectionner un modèle. Il se peut qu'un modèle qui présente une adéquation parfaite ne réussisse pas bien à prédire des événements futurs. Klugman donne donc deux conseils à garder en tête:\\
	
	\begin{itemize}
		\setlength\itemsep{0.5em}
		\item Utiliser autant que possible des modèles qui sont simples.
		\item Restreindre les modèles possibles.\\
	\end{itemize}

	Pour ce qui est du premier conseil, il s'agit de limiter le nombre de raccordements et/ou le nombre de paramètres. Un modèle qui s'apparente trop à ses données d'entrainement ne sera bon qu'à prédire ces mêmes données et n'aura aucun pouvoir de prédiction. C'est ce que l'on appelle du surentrainement statistique.\\
	
	Pour le deuxième conseil, il s'agit de rejeter d'emblée des modèles qui ne font pas de sens. De là vient l'importance de faire une bonne analyse préliminaire.\\
	
	Finalement, une fois ces informations à notre disposition, la dernière étape de la sélection de modèle consiste à faire appel à son expérience et/ou, à tout le moins, au \textit{gros bon sens}.
	En effet, il faut considérer le contexte et les implications des décisions résultant des modèles. Si on choisit un modèle qui sous-évalue les besoins en capital sous prétexte que ce modèle avait une meilleure adéquation statistique, il se peut que cela crée des problèmes de solvabilité ultérieurement. Surtout si on travaille dans un contexte de prédiction à long terme comme en assurance vie ou en gestion de régime de retraite.
